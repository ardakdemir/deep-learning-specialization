{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab374d43",
   "metadata": {},
   "source": [
    "## Word Representation\n",
    "\n",
    "Word Embeddings yay!\n",
    "\n",
    "- So far we used one-hot encoding for representing words/chars\n",
    "\n",
    "Motivating example:\n",
    "\n",
    "Man: $[0,1,....,0]$\n",
    "\n",
    "\n",
    "Woman: $[0,0,0,0,0,0,0,1,....,0]$\n",
    "\n",
    "\n",
    "- No way to associate similar words using one-hot encoding.\n",
    "- Thus no generalization across words/concepts... \n",
    "\n",
    "Solution:\n",
    "\n",
    "- Learning featurized representation: word embedding\n",
    "- dim=0 gender    man=1\n",
    "- dim=1 royal\n",
    "- dim=2 age\n",
    "- dim3= food    apple/orange have high value\n",
    "- ... dim300 ....\n",
    "\n",
    "\n",
    "Now, with this representations, apple and orange will have similar representations. \n",
    "\n",
    "\n",
    "Of course, manually choosing these definitions for each dimension is not a feasible option. Instead we let network learnig it.\n",
    "\n",
    "\n",
    "Use t-SNE, for visualizing the high dimensional (300dim) vector into 2D. \n",
    "\n",
    "The reason we say embedding is:\n",
    "\n",
    "- Imagine a 3D cube\n",
    "- Each point is the embedding of a word. So we embed each word in that X dimensional hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055e21e",
   "metadata": {},
   "source": [
    "## Using Word Embeddings\n",
    "\n",
    "\n",
    "How we can plug them into applications.\n",
    "\n",
    "\n",
    "Example Named Entity Recognition \n",
    "\n",
    "\n",
    "    Training:  Sally Johnson is an orange farmer. \n",
    "\n",
    "    Test: Robert Lin is a apple farmer (durian cultivator).\n",
    "\n",
    "If you have little training data you might not have seen durian in your named entity recognition training.\n",
    "\n",
    "\n",
    "Advantage of word embeddings is that, they are learned unsupervised on huge datasets, so durian embedding will be similar to orange.\n",
    "\n",
    "\n",
    "Setting:\n",
    "\n",
    "- 1B words for training word embeddings\n",
    "- 100k words for your named entity recognition task\n",
    "- So we employ transfer learning to utilize the word embeddings.\n",
    "- Also note that usually embeddings are much smaller in size (300) compared to OHE (10k vocab).\n",
    "- (Optional) Continue fine-tuning your word embeddings on the new data. This is only done if you have enough data.\n",
    "\n",
    "\n",
    "**Finally relation to Face Encoding**\n",
    "\n",
    "- For face recognition, we used Siamese networks which had image encodings and compared the similarity.\n",
    "\n",
    "- For word embeddings, we have a fixed vocabulary that we learn a fixed embedding. On the other hand, face recognition with CNNs allowed us to input any image to the embedding model. So we can not input \"afkdafgdslkafna;sd\" and try to get the embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af7def",
   "metadata": {},
   "source": [
    "## Properties of Word Embeddings\n",
    "\n",
    "There were interesting analogies using word embeddings. Classic example of \n",
    "\n",
    "$$e_{queen} = e_{king} - e_{man} + e_{woman}$$\n",
    "\n",
    "\n",
    "Implementation:\n",
    "\n",
    "```\n",
    "argmax sim(ew , eking - eman + ewoman)\n",
    "```\n",
    "\n",
    "You can see almost 70% analogy accuracy on an analogy dataset! You use the similarity of the full 300 dimensional embedding vectors\n",
    "\n",
    "\n",
    "Important note about t-SNE:\n",
    "\n",
    "- It maps the input vectors to 2D in a veeery non-linear complicated way.\n",
    "- After t-SNE, you can not use the conventional distance metrics on the t-SNE vectors. They are only for visualization.\n",
    "\n",
    "\n",
    "\n",
    "#### Common similarity metrics \n",
    "\n",
    "Cosine similarity.\n",
    "\n",
    "$$\n",
    "sim(u,v) = \\dfrac{u^{T}v}{\\lvert\\lvert u\\rvert\\rvert_2 \\cdot \\lvert\\lvert v\\rvert\\rvert_2}\n",
    "$$\n",
    "\n",
    "\n",
    "You can also use Euclidean distance but this is less frequently used\n",
    "\n",
    "\n",
    "$$\n",
    "\\lvert\\lvert u-v\\rvert\\rvert^2 \n",
    "$$\n",
    "\n",
    "\n",
    "Some analogy examples:\n",
    "\n",
    "- Man:Woman as Boy:Girl\n",
    "- Ottawa:Canada as Nairobi:Kenya\n",
    "- Currencies of countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5c7f4",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "\n",
    "Imagine again our same 10k vocabulary. \n",
    "\n",
    "We will have a 300x10k matrix $E$. Each column is the embedding representation.\n",
    "\n",
    "\n",
    "Notation:\n",
    "\n",
    "- $O_{6257}$ used to denote one hot encoding of 'orange'\n",
    "\n",
    "$$\n",
    "E \\cdot O_{6257} = \\text{embedding of orange <300,1>} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba27a17",
   "metadata": {},
   "source": [
    "## Learning Word Embeddings: Word2Vec & GloVe\n",
    "\n",
    "We will learn concrete algorithms to learn embeddings.\n",
    "\n",
    "Historically people proposed very complex algorithms to learn these embeddings. Over time the algorithms interestingly got simpler aaand simpler.\n",
    "\n",
    "\n",
    "Andrew goes over the Bengio 2003 paper Andrej karpathy also references in his playlist.\n",
    "\n",
    "Example:\n",
    "\n",
    "    I want a glass of orange ___________ . \n",
    "\n",
    "\n",
    "\n",
    "- Get the embedding of all words until the blank token.\n",
    "- Each are 300-dim vectors -> 6x300: 1800\n",
    "    - Alternatively we can also limit the context to last 4 words (context window length)\n",
    "- Input all these embeddings to a neural network.\n",
    "- Softmax with 10k outputs to predict the correct word!! \n",
    "\n",
    "\n",
    "This algorithm will be able to learn pretty good embeddings. It will learn that apple and orange are similar etc.\n",
    "\n",
    "\n",
    "\n",
    "However, above method is NOT the only way to train the embeddings. Let's look at other approaches.\n",
    "\n",
    "### Other context/target pairs\n",
    "\n",
    "example:\n",
    "\n",
    "    I want a glass of orange juic to go along with my cereal.\n",
    "    \n",
    "\n",
    "If your goal is to have a language model, then last N words as context makes sense. However, if you only want good representations given an input sequence you can also use left&right context:\n",
    "\n",
    "Context:\n",
    "\n",
    "- Last 4 words: This is the main way if we want to learn a language model\n",
    "- 4 words on left & right (predict the word in the middle): \n",
    "- Last 1 word (simpler context)\n",
    "- Nearby 1 word: Skipgram model which is found to work remarkably well\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c6a3d",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "**Skip-gram model**\n",
    "\n",
    "example:\n",
    "\n",
    "    I want a glass of orange juic to go along with my cereal.\n",
    "\n",
    "\n",
    "- Randomly pick a word from (-5,5 words) as the context word for a given target word.\n",
    "- Try to predict the target given the context\n",
    "- Goal is not really to do well on the prediction task but to learn good embeddings.\n",
    "\n",
    "\n",
    "Model.\n",
    "\n",
    "- Vocab size = 10k\n",
    "- learn mapping from context (c) to target (t) \n",
    "- start by OHE for context `c` and input the embedding to a network\n",
    "- output is the softmax unit to predict `t`\n",
    "\n",
    "$$\n",
    "P(t|c) = \\dfrac{e^{\\Theta_t^Te_c}}{\\text{total}}\\\\\n",
    "L  = - \\sum_{i=1}^{10k} y_i log(\\hat{y}_i))\n",
    "$$\n",
    "\n",
    "where $\\Theta$ is the output layer weights\n",
    "\n",
    "\n",
    "#### Problem with Softmax Classification\n",
    "\n",
    "Computation is small \n",
    "\n",
    "- For each sample we need to calculate a 10k sum to calculate the denominator. \n",
    "- One solution is toi use Hierarchical Softmax. Divide the full vocabulary into maaany binary classifications. So the size become $log(v)$.\n",
    "- Common words are on the top levels of hierarchical layers since they occur more frequently.\n",
    "\n",
    "\n",
    "Another solution is negative sampling which we will cover in next lecture.\n",
    "\n",
    "\n",
    "Last comment:\n",
    "\n",
    "\n",
    "**How to sample the context c?**\n",
    "\n",
    "Probably you want to avoid frequent context words such as \"the, of, a, and, to\" since they will appear so many times.\n",
    "\n",
    "You should use different heuristics to balance less frequent words and frequent words so you can learn good embeddings for your whole vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253aa553",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "Softmax denominator is slow to compuuute.\n",
    "\n",
    "Negative sampling helps you to do something very similar to skip gram but do much faster.\n",
    "\n",
    "\n",
    "    I want a glass of orange juice to go along with my cereal.\n",
    "    \n",
    "\n",
    "- Generate a positive pair of context and target. For example: orange (c) and juice (t)\n",
    "    - label = 1\n",
    "- Generate k=(5-20) negative example pairs: Randomly pick words from your vocabulary for the target\n",
    "    - orange (c) and king (t)\n",
    "    - ....\n",
    "    - It is fine if your randomly selected `t` actually appears in the sentence. \n",
    "    - label for these pairs is 0\n",
    "- Define a supervised task:\n",
    "    - Given all our context words and target words try to predict whether target is 0 or 1.\n",
    "    \n",
    "    \n",
    "### Model.\n",
    "\n",
    "Instead of using a softmax now we will have logistic regression for each sample.\n",
    "\n",
    "\n",
    "$$\n",
    "P(y=1|c,t) = \\sigma(\\Theta_t^{T} e_c)\n",
    "$$\n",
    "\n",
    "We will have a k:1 ratio of negative to positive labels (y=0 vs y=1).\n",
    "\n",
    "\n",
    "One way to look at it: Instead of trying to make a 10k class prediction at once, we formulate the task as 10k binary classification tasks (very few positive and many negative samples). And we only train on `k+1` samples instead of the whole set. So much much faster.\n",
    "\n",
    "\n",
    "\n",
    "#### Selecting the negative examples\n",
    "\n",
    "\n",
    "Some approaches:\n",
    "\n",
    "- Sample proportional to occurrence frequency: common words are sampled way too often\n",
    "- Sample uniform randomly: rare words are sampled many times\n",
    "- Authors proposal: Somewhere in the middle:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\dfrac{f(w_i)^{3/4}}{\\sum_j f(w_j)^{3/4}}\n",
    "$$\n",
    "\n",
    "where $f(w_i)$ is the frequency of $w_i$ in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa43a67",
   "metadata": {},
   "source": [
    "## GloVe word Vectors\n",
    "\n",
    "Some momentum recently hehe\n",
    "\n",
    "Name: Global vectors for word representation!!\n",
    "\n",
    "The core idea is, instead of predicting binary as in skip-gram, let's generalize this cooccurrence idea to all pairs and try to predict the count instead.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "First we create a matrix $\\mathcal{X}$ storing the counts \n",
    "\n",
    "$$\n",
    "\\mathcal{X}_{ij} = \\text{# of times j (t) appears in the context of i (c)}\n",
    "$$\n",
    "\n",
    "Depending on the definition of `context` we might even have $\\mathcal{X}_{ij}=\\mathcal{X}_{ji}$. For example, context might mean in the vicinity of 10 words. this is a symmetric relationship. If context means the word that preceeds the target then it is NOT.\n",
    "\n",
    "\n",
    "\n",
    "GloVe try to minimize the following expression:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{10k} \\sum_{j=1}^{10k} f(\\mathcal{X}_{ij}) (\\Theta_i^{T} e_j +b_i + b_j    - log(\\mathcal{X}_{ij}))^2 \n",
    "$$\n",
    "\n",
    "$log(0)$ is undefined but for those we will handle them using 0 weight for those samples. So $f(\\mathcal{X}_{ij})=0$ if $\\mathcal{X}_{ij}=0$. and assume $0 * log(0) = 0$.\n",
    "\n",
    "\n",
    "Weighting term ($f(\\mathcal{X}_{ij})$) also helps with the following:\n",
    "\n",
    "- For common frequent words 'this, is, of, a' we want to give large weight but not unduly large so they dominate the training too much.\n",
    "- Give meaningful amount of weight to rare words so we can learn them too. Words such as Durian.\n",
    "\n",
    "\n",
    "**Something I don't fully comprehend**\n",
    "\n",
    "Andrew says $\\Theta_i$ and $e_j$ are symmetric. So that we can initialize these matrices at the beginning randomly. However, when we finish the training we can average them to get the final embeddings \n",
    "\n",
    "$$\n",
    "e_{j}^{final} = \\dfrac{e_j + \\Theta_j}{2}\n",
    "$$\n",
    "\n",
    "Turns out this simple approach of just minimizing the distance between number of occurrences actually works quite well.\n",
    "\n",
    "\n",
    "I think overall Andrew skipped many details about the paper and it was very confusing. Like he didn't even bother mentioning why do we have the $b_i$ and $b_j$ terms.\n",
    "\n",
    "Paper is god: https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "**Okay Got it!!**\n",
    "\n",
    "I was confused a lot about the part that $\\Theta$ and $e$ are symmetric. It is actually quite straightforward. Because in this GloVe algorithm the definition of context is symmetric: word j appears in the +-10 of word i. So the X matrix have symmetry in its entries. This means that the two parameter sets will try to learn the exactly same symmetrical values.\n",
    "\n",
    "#### A note on the featurization view of word embeddings\n",
    "\n",
    "When we learn embeddings in GloVe or Word2Vec way, we can NOT expect that individual dimensions actually correspond to something meaningful. The axes will not be orthogonal to the concepts we have. \n",
    "\n",
    "Some linear algebra:\n",
    "\n",
    "$$\n",
    "(A\\Theta_i)^T(A^{-T}e_j) = \\Theta_i^{T} e_j\n",
    "$$\n",
    "\n",
    "Above proves that if we had the above transformation $A$, which proves there could be some other axis from what we expect to be true (human interpretable). But the analogy idea still works even though individual axes don't overlap.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2b964",
   "metadata": {},
   "source": [
    "## Sentiment Classification\n",
    "\n",
    "Look at a piece of text and predict the sentiment.\n",
    "\n",
    "With using embeddings, you can train good sentiment classifier with minimal data yay!!\n",
    "\n",
    "    Example: The dessert is excellent              -> 4 stars\n",
    "\n",
    "1. Simple model.\n",
    "\n",
    "- Take OHE for each word and get the embeddings\n",
    "- Sum or average the embeddings to get 300dim vector.\n",
    "- Pass to softmax and output between 1-5\n",
    "\n",
    "Problem: lacks the word order information. So fails to detect negative case like \"lacking in good service\" because good looks like positive\n",
    "\n",
    "2. More sophisticated model: RNN\n",
    "\n",
    "- Take OHE for each word and get the embeddings.\n",
    "- Feed all embeddings to an RNN.\n",
    "- Many-to-one architecture and last word works to predict the rating.\n",
    "\n",
    "Embeddings help us generalize to words that don't appear in our sentiment classification fine-tuning dataset which could be small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6dbba7",
   "metadata": {},
   "source": [
    "## Debiasing Word Embeddings\n",
    "\n",
    "Problem: Learn biased representations from the training data!!\n",
    "\n",
    "Bias here means bias around gender or ethnicity etc. Yabai examples:\n",
    "\n",
    "    Man:Computer_Programmer as Woman:Homemaker\n",
    "    Father:Doctor as Mother:Nurse\n",
    "    \n",
    "    \n",
    "    \n",
    "Embeddings reflect the gender, ethnicity, age, sexual orientation and other biases of the text used to train the model!!!\n",
    "\n",
    "**How can we avoid this?**\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1607.06520.pdf \n",
    "\n",
    "I LOVED THE IDEA!!!\n",
    "\n",
    "\n",
    "Three steps:\n",
    "\n",
    "- Identify the bias direction: One bias at a time. Example: gender\n",
    "    - We do this by subtracting many pairs as below and averaging them:\n",
    "        - $e_{he} - e_{she}$\n",
    "        - $e_{male} - e_{female}$\n",
    "        - ...\n",
    "        - average all these differences to get the bias direction.\n",
    "- Neutralize: For every word that is not definitionally related to that concept (in our case gender), project to get rid of the bias.\n",
    "    - So definitionally related words for gender are words like 'man' 'father' etc.\n",
    "    - This helps us avoid `doctor` to have any gender related value. Gender dimension for doctor, soccer player, nurse will all be 0 (meaning it is neutral).\n",
    "- Equalize: For all pairs of words where the only difference is gender (grandmother-grandfather or boy-girl), Equalize their vectors in the remaining 299-dimensions.\n",
    "    - This helps us avoid grandmother being closer to nurse than grandfather \n",
    "    \n",
    "    \n",
    "\n",
    "How do we choose which words to neutralize in step 2? \n",
    "\n",
    "Authors train a classifier to decide whether words are definitional or not. And find out that many words are not definitional. Hmmm what kind of classifier?\n",
    "\n",
    "Also for step 3, pairs we equalize are also apparently small in size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154849b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f79afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093640f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e204e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aaa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4d606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b80ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb645180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526bf967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f8e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f68d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a9c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fefbdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbdf2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bd0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f800ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed3ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4436dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704c91a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec9c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093698a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc171f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f2151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbd89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbe768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e18ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b458b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d9496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d80ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca65c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1709240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56f4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c9416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc019160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfbd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2626ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7763e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68a6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de103c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febed7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464629f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-karpathy",
   "language": "python",
   "name": "venv-karpathy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
