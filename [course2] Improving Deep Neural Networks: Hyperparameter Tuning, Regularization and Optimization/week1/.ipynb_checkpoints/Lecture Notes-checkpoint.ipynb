{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783975a2",
   "metadata": {},
   "source": [
    "## Train/dev/test sets\n",
    "\n",
    "Regarding split size:\n",
    "- We used to have 70/30 or 60/20/20 in traditional machine learning. \n",
    "- However in Big Data era, we have giaaant dataset. So we can have 98/1/1 or 99/0.5/0.5 splits.\n",
    "- Full dataset is so large that we don't need 20% to get coverage. 1% is more than enough.\n",
    "\n",
    "Mismatch train/test:\n",
    "\n",
    "- Training set coming from webpages -> dev/test coming from app camera\n",
    "- Rule of thumb: Dev and test are coming from same distribution: Then you can capture the issues at dev stage.\n",
    "\n",
    "\n",
    "If you iterate on the model based on the results on the `test` set, it can not be called an unbiased estimator anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6c85e",
   "metadata": {},
   "source": [
    "## Bias/Variance\n",
    "\n",
    "Important to master!! Less tradeoff in the deep learning era.\n",
    "\n",
    "- High bias: Underfitting (can not even perform well on training set)\n",
    "- High variance: Overfitting (huge performance drop going from training to test)\n",
    "\n",
    "Ideally we want low bias and low variance. There are cases where model is both high bias and high variance. \n",
    "This could be the case when our model is complex and actually complex in a bad way. So performance is low on training and still gets even worse on the dev set.\n",
    "\n",
    "\n",
    "Optimal (Bayes) error: Is the error we accept as \"good\" error rate\n",
    "\n",
    "- Optimal error is required for us to judge whether a model is underfitting (performing bad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712b6be",
   "metadata": {},
   "source": [
    "## Basic Recipe for ML \n",
    "\n",
    "1. High bias? (looking at the training set performance only) Try: \n",
    "- Bigger network\n",
    "- Train longer\n",
    "- NN architecture search\n",
    "\n",
    "Try until you achieve low bias\n",
    "\n",
    "2. Next, high variance? (looking at the dev set performance) Try: \n",
    "\n",
    "- More data\n",
    "- Regularization\n",
    "- NN architecture search (?)\n",
    "\n",
    "Try these until you lower the variance and go back to step 1.\n",
    "\n",
    "\n",
    "For example, if you have high bias, training with more data would probably not help.\n",
    "\n",
    "\n",
    "\n",
    "### Why no bias/variance tradeoff in DL era?\n",
    "\n",
    "- In old days, usually lowering one meant increasing the other.\n",
    "- More data usually results in lowering the variance.\n",
    "- Larger model usually means lowering the bias.\n",
    "- So in DL era we don't have to always have a tradeoff between the two.\n",
    "- One of the main reasons why DL with supervised learning was very successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de1afb",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "One of the main things to try when high variance is regularization.\n",
    "\n",
    "Logistic regression case (only one $w$ and $b$):\n",
    "\n",
    "- $L_2$ regularization: $\\sum_{j=1}^{n_x} w_{j}^2$  \n",
    "    - Why only w and not b? Because b is small # of params compared to w.\n",
    "- $L_1$ regularization: W will end up being sparse (a lot of zeros). \n",
    "- $\\lambda$ is a regularization parameter which is a hyperparam we have to tune\n",
    "\n",
    "\n",
    "Neural Network case: \n",
    "\n",
    "- sum over all network weight params\n",
    "- It is called Frobenius Norm (not L2 norm) when its over matrix elements. Elementwise square summed up.\n",
    "- $L_2$ is also called weight decay. because gradient will always make the $W$ matrix slightly smaller.\n",
    "    - This is achieved because the weight matrix is always multiplied by a number slightly smaller than 1.\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63611f5b",
   "metadata": {},
   "source": [
    "## Why regularization reduces overfitting?\n",
    "\n",
    "- Imagine the tanh graph. It looks like a linear function around 0. \n",
    "\n",
    "- So if you actually introduce regularization which makes your weights close to 0, the function becomes much simpler. Almost like a linear function. Thus avoid overfitting. \n",
    "\n",
    "- We can not have crazy decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c5253",
   "metadata": {},
   "source": [
    "## Dropout Regularization\n",
    "\n",
    "\n",
    "How to implement dropout?? \n",
    "\n",
    "### Inverted dropout\n",
    "\n",
    "1) Have a matrix that denotes whether we drop a matrix cell.\n",
    "2) Divide it by the keep prob so that the outcome total value is not expected to go down!!\n",
    "\n",
    "**Test Time**\n",
    "\n",
    "- No drop out !!\n",
    "- Since we had the scaling up with inverted dropout during training, we can be confident that when we turn off the dropout, the values do NOT become too high.\n",
    "- In old days, some implementations of dropout excluded this inverted division. \n",
    "- That caused problem during inference time because we have to scale down the weights when dropout is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeef3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "keep_prob = 0.5\n",
    "\n",
    "# drop out\n",
    "d3= np.random.rand(3,2) < keep_prob\n",
    "a3 = np.random.rand(3,2)\n",
    "\n",
    "a3 = np.multiply(a3, d3)\n",
    "\n",
    "#inverted scaling\n",
    "a3/=keep_prob # which is essentially scaling the values up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb578e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60380948, 0.        ],\n",
       "       [0.1673716 , 0.76804806],\n",
       "       [0.00951474, 1.21607543]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fdbbdf",
   "metadata": {},
   "source": [
    "## Understanding Dropout\n",
    "\n",
    "Intuition: Cant rely on any one feature, so have to spread out weights.\n",
    "\n",
    "- For huuge layers with many params maybe we can increase the dropout rate.\n",
    "- Usually dropping out input features is not good idea.\n",
    "- Dropout usually not needed as long as there is no overfitting on the training data.\n",
    "- With Dropout, your cost function is not really well-defined anymore, since we cancel different cells at each iteration.\n",
    "- Good practice: Turn off dropout. Confirm the cost monotonically decreases, then repeat with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77bdb1c",
   "metadata": {},
   "source": [
    "## Other Regularization Methods\n",
    "\n",
    "- Data Augmentation: Flip images horizontally.\n",
    "- Early stopping: stop when dev set error stops improving.\n",
    "\n",
    "\n",
    "### Downside of Early Stopping\n",
    "\n",
    "**Orthogonalization Concept**\n",
    "\n",
    "It is the idea that at any given time, you only focus on one specific goal: Reduce overfitting OR optimize cost function.\n",
    "\n",
    "However with Early stopping, we kind of make it more complex to focus on one task because early stopping aims to `regularize` the weights by stopping the other goal of minimizing the cost function.\n",
    "\n",
    "Andrew NG recommends $L_2$ as a more standard way of regularization for this specific reason.\n",
    "\n",
    "Benefits: No need to do additional hyperparam search or computation. It is kinda win-win: Less training and better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b019b6",
   "metadata": {},
   "source": [
    "## Normalizing Inputs\n",
    "\n",
    "1. Subtract mean\n",
    "2. Normalize variance: Divide by the standard deviation square. \n",
    "\n",
    "\n",
    "This helps to have all input features have roughly uniform scaling. You don't have one input with huuuge variance and other very small variance.\n",
    "\n",
    "3. **!!Important!!** Always use the same mean and variance that you used for training on the test set!! Do not calculate it separately.\n",
    "\n",
    "\n",
    "Below are the steps\n",
    "\n",
    "1. Calculate the population mean and std: \n",
    "\n",
    "$$\n",
    "\\mu = \\dfrac{\\sum_{i=1}^{N} x_i}{N}\\\\\n",
    "var = \\dfrac{\\sum_{i=1}^{N} (x_i-\\mu)^2}{N}\\\\\\\\\n",
    "\\sigma = \\sqrt{var}\n",
    "$$\n",
    "\n",
    "\n",
    "2. Normalize:\n",
    "\n",
    "$$\n",
    "x_i = \\dfrac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "**Why normalize??**\n",
    "\n",
    "If scale is different lets say one input ranges (0,1) and other (0,1000) it will be difficult to train.\n",
    "It just works way better when they are usually centered around 0 and have similar scale\n",
    "\n",
    "Even if your input params have similar scale it almost never hurts to normalize your inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eacf7f",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients\n",
    "\n",
    "Vanishing case: \n",
    "\n",
    "\n",
    "Imagine you have 20 layer network and all weights are slightly lower than 1. \n",
    "Then at each layer, the activation of the neurons will get smaller and smaller. \n",
    "Which means the effect of lower layer get super small (so vanishing gradients).\n",
    "\n",
    "\n",
    "Exploding case:\n",
    "\n",
    "Same with values slightly higher than 1!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63b163",
   "metadata": {},
   "source": [
    "## Weight Initialization for Deep Networks\n",
    "\n",
    "**Basically kinda solution to vanishing/exploding gradients**\n",
    "\n",
    "Consider a single neuron. It is a weighted combination of all input features:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} x_i w_i\n",
    "$$\n",
    "\n",
    "So if we have n as a veeery big number we want $w_i$'s to be smaller.\n",
    "\n",
    "As a solution, when initializing the weights we can divide with some number that increases with value of $n$.\n",
    "\n",
    "\n",
    "If Relu activation is used, it is recommended to divide by square root of $\\dfrac{2}{n}$. One possible reason might be that relu kills the half of the values? \n",
    "\n",
    "\n",
    "If tanh is used (Xavier Initialization) we are recommended to use:\n",
    "\n",
    "$$\n",
    "\\sqrt{\\dfrac{1}{n}}\n",
    "$$\n",
    "\n",
    "\n",
    "Some also use:\n",
    "\n",
    "$$\n",
    "\\sqrt{\\dfrac{2}{n + n_{+1}}}\n",
    "$$\n",
    "\n",
    "where ${n_{+1}}$ just means the size of the output of this layer.\n",
    "\n",
    "\n",
    "\n",
    "Overall, this ensures that weights are neither too small nor too big. Just around 1 so that we don't get vanishing and exploding gradients.\n",
    "\n",
    "\n",
    "\n",
    "### More explanation\n",
    "\n",
    "Ref:https://www.deeplearning.ai/ai-notes/initialization/index.html\n",
    "\n",
    "To prevent the gradients of the networkâ€™s activations from vanishing or exploding, we will stick to the following rules of thumb:\n",
    "\n",
    "1- The mean of the activations should be zero.   \n",
    "2- The variance of the activations should stay the same across every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fa6d8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.432054162608742\n",
      "0.1904579926064798\n",
      "-0.11629760175749276\n"
     ]
    }
   ],
   "source": [
    "# Code example\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "input_dim = 2000\n",
    "Xs = np.random.randn(input_dim,1)\n",
    "\n",
    "# Random initialization\n",
    "W = np.random.randn(30,input_dim)\n",
    "print(np.mean(np.matmul(W,Xs)))\n",
    "\n",
    "# Xavier Initialization \n",
    "W = np.random.randn(30,input_dim) * np.sqrt(1/input_dim)\n",
    "print(np.mean(np.matmul(W,Xs)))\n",
    "# He Initialization \n",
    "W = np.random.randn(30,input_dim) * np.sqrt(2/input_dim)\n",
    "print(np.mean(np.matmul(W,Xs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfdbda3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89742b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see\n",
    "import numpy as np\n",
    "nin = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005a3aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41120098,  0.46342294],\n",
       "       [-0.3797018 , -0.87384133],\n",
       "       [-0.53344218,  0.22861401]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a087b",
   "metadata": {},
   "source": [
    "## Numerical Approximation of Gradients\n",
    "\n",
    "How to check your backpropagation is correct? \n",
    "\n",
    "lets take f(x) = x^3\n",
    "\n",
    "instead of just nudging to right and computing the gradient, it is better to consider $\\theta + \\epsilon$ and $\\theta - \\epsilon$ (something like a larger triangle that consider both directions).\n",
    "\n",
    "This should yield a better approximation to the actual value of the derivative.\n",
    "\n",
    "\n",
    "\n",
    "Optional Theory:\n",
    "\n",
    "\n",
    "There are two potential definitions of limit:\n",
    "\n",
    "two-sided derivative: \n",
    "\n",
    "\n",
    "$$\n",
    "\\dfrac{f(\\theta+\\epsilon)-f(\\theta-\\epsilon)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "Then the error rate is on the order $O(\\epsilon^2)$.\n",
    "\n",
    "But if we take one-sided derivative, error is $O(\\epsilon)$. \n",
    "\n",
    "Since $\\epsilon$ is very small its difference with its square is huuuge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b425695",
   "metadata": {},
   "source": [
    "## Gradient Checking \n",
    "\n",
    "Put aaalll your parameters in a single dimensional giant vector $\\Theta$.\n",
    "\n",
    "Then for all params, take the two-sided gradient checking and store them in $d\\Theta_{approx}$\n",
    "\n",
    "Calculate $L_2$ norm of difference between the actual gradient vector, $d\\Theta$, and the approximation, divided by the sizes of each vector.\n",
    "\n",
    "If it is below $10^{-5}$ or $10^{-7}$ then your backprop algorithm is likely correct!!\n",
    "\n",
    "\n",
    "This is kinda similar to what Andrej Karpathy does in his zero-to-hero lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca40c3",
   "metadata": {},
   "source": [
    "## Gradient checking implementation notes\n",
    "\n",
    "- Dont use in training of course!! It is slow, that's just for debugging.\n",
    "\n",
    "- If check fails, look at individual components that are different to identify the bug. \n",
    "\n",
    "- If all coming from $dW^{[l]}$ then look there\n",
    "\n",
    "- Remember regularization when implementing grad check, that should be included in your approximation.\n",
    "\n",
    "- Wont work with dropout of course! Implement without dropout!!\n",
    "\n",
    "- Dont just run the grad check at the very beginning. It might be the case that for small values of $w$ and $b$ your backprop works fiine. BUT for larger values, there might be an issue. So Andrew recommends us to run once at the beginning and perhaps again after some training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f0ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41fde6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91060789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6b7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81718c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-karpathy",
   "language": "python",
   "name": "venv-karpathy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
