{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5bb934",
   "metadata": {},
   "source": [
    "## \\[Assignment 1\\] Initialization \n",
    "\n",
    "#### 1. Zeros initialization\n",
    "\n",
    "In this case, due to loss being backpropagated identically by symmetry, we wont be utilizing the hidden units.\n",
    "\n",
    "The network simply becomes a thin and deep string like network.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- The weights $W^{[l]}$ should be initialized randomly to break symmetry. \n",
    "- However, it's okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95167679",
   "metadata": {},
   "source": [
    "#### 2.  Random initialization with large values\n",
    "\n",
    "**Observations**:\n",
    "\n",
    "- When we initialize the weights to be super large values, it is veeery likely that sigmoid output of last layer is either very close to 1 ooor very close to 0 (think of sigmoid of -1000, or 1000). \n",
    "- This causes our loss to be veery high if we get things wrong. And in random initialization we do expect to get half of the samples wrong. This results in $inf$ loss which is not good. \n",
    "\n",
    "- Poor initialization leads to vanishing/exploding gradients.\n",
    "\n",
    "- Eventually we might get good results but initializing with high values causes training convergence to slow down.\n",
    "\n",
    "\n",
    "\n",
    "**Why randn (standard normal) and not rand (uniform)?**\n",
    "\n",
    "- Well think of the uniform sampler. Then significant number of weights will be in the extremes (such as very close to -1 or close to 1). \n",
    "\n",
    "- This results in the last layer sigmoid output to have a veery small gradient (because sigmoid gradient is only good around the center which is near 0 value).\n",
    "\n",
    "- So if we use randn most weights will be around the center, and this will yield larger gradient values at the beginning of the training.\n",
    "\n",
    "- Uniform would yield a much slower convergence on the other hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d501e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically check what happens when\n",
    "import numpy as np\n",
    "scale = 10\n",
    "w_l = np.random.randn(3,2) * scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12b925d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.37196125, -10.52565532],\n",
       "       [ 27.63194141, -13.91045437],\n",
       "       [ -2.2577012 ,  -7.41340445]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53faf109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92864105],\n",
       "       [0.7002517 ],\n",
       "       [0.38039037],\n",
       "       [0.103201  ],\n",
       "       [0.09608372],\n",
       "       [0.15344371],\n",
       "       [0.50486605],\n",
       "       [0.78872799],\n",
       "       [0.21165572],\n",
       "       [0.68714761]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(10,1) #range is [0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b631d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4394421 ],\n",
       "       [ 0.24302633],\n",
       "       [ 1.2852578 ],\n",
       "       [-0.19889892],\n",
       "       [ 1.09327393],\n",
       "       [ 0.12481458],\n",
       "       [-0.21623744],\n",
       "       [ 0.70704534],\n",
       "       [-1.81829869],\n",
       "       [-0.17450156]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(10,1)  #range is arbitrary, standard normal (mean=0,std=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bc866",
   "metadata": {},
   "source": [
    "**Congratulations**! You've completed this notebook on Initialization. \n",
    "\n",
    "Here's a quick recap of the main takeaways:\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "- Different initializations lead to very different results\n",
    "- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
    "- Resist initializing to values that are too large!\n",
    "- He initialization works well for networks with ReLU activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde76b5",
   "metadata": {},
   "source": [
    "## \\[Assignment 2\\] Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e9a15",
   "metadata": {},
   "source": [
    "**$L_2$ Regularization**\n",
    "\n",
    "<br>\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember:** the implications of L2-regularization on:\n",
    "- The cost computation:\n",
    "    - A regularization term is added to the cost.\n",
    "- The backpropagation function:\n",
    "    - There are extra terms in the gradients with respect to weight matrices.\n",
    "- Weights end up smaller (\"weight decay\"): \n",
    "    - Weights are pushed to smaller values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ed791",
   "metadata": {},
   "source": [
    "    \n",
    "    \n",
    "**Dropout**\n",
    "    <br>\n",
    "    \n",
    "Critical observation: When you shutdown some neurons, you actually modify the model. So at every iteration, you train a subset (new) model. Dropout helps with overfitting, intuitively because it teaches every neuron to be less sensitive to the input become, they might disappear on other iteration. Quoting the assignment notes:\n",
    "   \n",
    "    \"When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.\"\n",
    "    \n",
    "\n",
    "    \n",
    "Reminder: When you apply dropout to a layer do NOT forget to scale it up by dividing it with keep_prob!!\n",
    "\n",
    "\n",
    "<br>\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember about dropout:**\n",
    "- Dropout is a regularization technique.\n",
    "- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.\n",
    "- Apply dropout both during forward and backward propagation.\n",
    "- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beed2d5",
   "metadata": {},
   "source": [
    "## \\[Assignment 3\\] Gradient Checking\n",
    "\n",
    "We want to make sure our backpropagation algorithm works fine.\n",
    "\n",
    "So we can numerically approximate the gradient of each parameter using the definition of derivative!\n",
    "\n",
    "**Notes** \n",
    "- Gradient Checking is slow! Approximating the gradient with $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ is computationally costly. For this reason, we don't run gradient checking at every iteration during training. Just a few times to check if the gradient is correct. \n",
    "- Gradient Checking, at least as we've presented it, doesn't work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout. \n",
    "\n",
    "Congrats! Now you can be confident that your deep learning model for fraud detection is working correctly! You can even use this to convince your CEO. :) \n",
    "\n",
    "\n",
    "<br>\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember from this notebook**:\n",
    "- Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).\n",
    "- Gradient checking is slow, so you don't want to run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca1655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63204db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1ae2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458df2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d5014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263e193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9648842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-karpathy",
   "language": "python",
   "name": "venv-karpathy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
