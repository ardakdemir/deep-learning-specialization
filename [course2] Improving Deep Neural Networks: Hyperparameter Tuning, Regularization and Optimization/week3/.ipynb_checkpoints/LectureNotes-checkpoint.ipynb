{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92779de5",
   "metadata": {},
   "source": [
    "# Part1. Hyperparameter Tuning\n",
    "\n",
    "## Tuning Process\n",
    "\n",
    "Soo many params to tune for deep learning.\n",
    "\n",
    "Top priority:\n",
    "\n",
    "- learning rate\n",
    "\n",
    "\n",
    "Second priority:\n",
    "\n",
    "- momentum term ($\\beta$)\n",
    "- mini-batch size \n",
    "- number of hidden units\n",
    "\n",
    "\n",
    "Third (low) priority:\n",
    "\n",
    "- number of layers\n",
    "- learning rate decay\n",
    "\n",
    "\n",
    "Almost never tune:\n",
    "\n",
    "- Adam params: $\\beta_1 (0.9)$, $\\beta_2 (0.999)$, $epsilon (10^{-8})$\n",
    "\n",
    "\n",
    "\n",
    "### How do you select values to explore? \n",
    "\n",
    "\n",
    "#### Random > Grid\n",
    "\n",
    "In earlier days, we did like grid search and systematically explore the values. Works okay when # of params are small\n",
    "\n",
    "\n",
    "Now it is better to choose randomly!! Reason:\n",
    "\n",
    "- Difficult to know in advance which params are critical.\n",
    "- If we use grid we will try way less distinct values for each param.\n",
    "- If for example $\\epsilon$ has little impact in your results, you might test 25 combinations (5x5 for two params) and you only explored 5 distinct values for the other param (e.g., learning rate)\n",
    "\n",
    "\n",
    "#### Coarse to Fine\n",
    "\n",
    "- You can first explore the whole region \n",
    "- If you find a specific region that works well, sample more densely in the zoomed in region.\n",
    "- Sounds like explore vs exploit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e2e3a",
   "metadata": {},
   "source": [
    "## Using an Appropriate Scale to pick Hyperparams\n",
    "\n",
    "Sampling randomly in the above notes doesn't mean we should always sample uniformly at random!!\n",
    "\n",
    "It is important to select the correct scale.\n",
    "\n",
    "To illustrate, if your hyperparameter range is from 2 to 5, it is fine to sample randomly or even do grid search.\n",
    "\n",
    "But how about for $\\alpha = [0.0001, 1]$ range. In this case, it doesn't make sense to sample uniformly since 90% of the attempts will be in the region $[0.1 - 1]$.\n",
    "\n",
    "In such cases, we should sample from the log scale. Below is implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93abcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = -4\n",
    "r = a * np.random.rand() \n",
    "alpha = 10**r \n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465ce0f",
   "metadata": {},
   "source": [
    "As a general rule, if you want to sample from $10^a$ to $10^b$ in the logarithmic scale:\n",
    "\n",
    "- Sample uniformly at random in range $r \\in [a,b]$\n",
    "- Then set your param to $alpha=10^{r}$\n",
    "\n",
    "\n",
    "#### Sampling the hyperparam for exponentially weighted averages\n",
    "\n",
    "We usually want to sample $\\beta \\in [0.9,0.999]$. In this case, we use the same logarithmic scale for $(1-\\beta)$. \n",
    "\n",
    "So that we can spend equal amount of resources for the range $[0.99,0.999]$ as you spend in $[0.9,0.99]$!!\n",
    "\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "- For values very close to 1, small changes to $\\beta$ actually make a big differnce:\n",
    "    - Consider $0.9000 -> 0.9005$. Difference would be minimal\n",
    "    - But for $0.999 -> 0.9995$, the difference (impach of the change) is quite big."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca248f",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning in Practice\n",
    "(Pandas vs Caviar)\n",
    "\n",
    "Tips and tricks for hyperparam search process..\n",
    "\n",
    "\n",
    "- Intuitions do get stale: Re-evaluate occasionally. Do not assume that certain params simply work better..\n",
    "\n",
    "\n",
    "Two schools of thought: \n",
    "\n",
    "- Babysitting one model\n",
    "- Train many models in parallel\n",
    "\n",
    "\n",
    "**Babysitting one model**\n",
    "\n",
    "(panda approach of raising kids )\n",
    "\n",
    "- Everyday you look at the results and manually set the next values over many course of days.\n",
    "\n",
    "- This is necessary if you have limited compute and have to find good region without many runs.\n",
    "\n",
    "\n",
    "**Training many models in parallel**\n",
    "\n",
    "(caviar approach: millions of eggs and dont pay attention)\n",
    "\n",
    "- Train many models in parallel\n",
    "- Do not pay too much human attention to babysitting each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8450ec",
   "metadata": {},
   "source": [
    "# Part 2. Batch Normalization\n",
    "\n",
    "\n",
    "## Normalizing Activations in a Network\n",
    "\n",
    "Batch normalization:\n",
    "\n",
    "- Makes your model much more robust to small hyperparameter changes\n",
    "- Also makes it easier to train much deeper models\n",
    "\n",
    "Lets see how it works...\n",
    "\n",
    "\n",
    "- Remember that normalizing your input features helps speed up your learning. This was the case because it makes your gradient contours to have much more circular shape.\n",
    "\n",
    "\n",
    "How about the inner activations?? \n",
    "\n",
    "- Would be nice if we could normalize the activations (which are inputs to deeper layers). How can we do that? That would help deeper layers train faster.\n",
    "\n",
    "- Technically, we will normalize the outputs before the activation. So we normalize $Z^{[l]}$ not $A^{[]}$.\n",
    "\n",
    "\n",
    "#### Implementing Batch Norm.\n",
    "\n",
    "- Given some values from a specific layer $Z^{[l]}$: $Z^{[l](1)},Z^{[l](2)},..., Z^{[l](m)}$ Compute the mean:\n",
    "\n",
    "$$\n",
    "\\mu = \\dfrac{1}{m} \\sum_i Z^{[l](i)}\\\\\n",
    "\\sigma = \\dfrac{1}{m} \\sum_i (Z^{[l](i)}-\\mu)^2\\\\\n",
    "Z_{norm}^{[l](i)} = \\dfrac{Z^{[l](i)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}\n",
    "$$\n",
    "\n",
    "$\\epsilon$ is added for numerical stability.\n",
    "\n",
    "\n",
    "Sometimes we don't want hidden units to have mean=0 and var=1. In that case what we do is:\n",
    "\n",
    "\n",
    "$$\\tilde{Z^{[l](i)}} = \\gamma^{[l]} Z_{norm}^{[l](i)} + \\beta^{[l]}$$\n",
    "\n",
    "where $\\gamma^{[l]}$ and $\\beta^{[l]}$ are learnable parameters for layer $l$. $\\gamma$ adjusts the variance, $\\beta$ adjusts the mean. Plug the values and try it yourself!!\n",
    "\n",
    "\n",
    "Intuition: We know normalizing inputs is helpful. So we use batch norm to normalize the hidden layer outputs. The only difference is that we don't ALWAYS want to fully normalize to mean=0 and variance=1. we use the gamma and beta params so that they can be adjustable. Anyway, this normalization helps with standardizing your inner layer outputs!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785d37e",
   "metadata": {},
   "source": [
    "## Fitting Batch Norm into a Neural Network\n",
    "\n",
    "With batch norm:\n",
    "\n",
    "- $Z^{[1]} = W^{[1]} X + b^{1}$\n",
    "- $\\tilde{Z^{[1]}}= \\gamma Z_{norm}^{[l](i)} + \\beta$\n",
    "- $A^1 = g^{[1]}(\\tilde{Z^{[1]}}) $\n",
    "\n",
    "and same for every layer. So parameters of your network are:\n",
    "\n",
    "- for every layer: $\\beta$ and $\\gamma$: $\\beta^{1},\\beta^{2},\\beta^{3},\\beta^{4}, \\gamma^{1}$\n",
    "- in addition to your $W$ and $b$ parameters\n",
    "- You will also train these new parameters using gradient descent!! \n",
    "\n",
    "$$\n",
    "\\beta^{[l]} = \\beta^{[l]} -  \\alpha * d\\beta^{[l]} \n",
    "$$\n",
    "\n",
    "Both $\\beta^{[l]}$ and $\\gamma^{[l]}$ have also dimension $(n^{[l]},1)$ similar to the bias term. This is the case because we want to learn a separate learnable parameter for scaling for each hidden unit!\n",
    "\n",
    "So after calculating $Z^{[l]}$ which is of shape  $(n^{[l]},m)$ where m is the batch size, we multiply by the $\\gamma$ scaling factor and add the $\\beta$ bias factor to each hidden unit.\n",
    "\n",
    "Note that usually in modern libraries we usually never implement the batch norm operation such as pytorch or (tf.nn.batch_normalization).\n",
    "\n",
    "\n",
    "#### In practice batch norm is applied with mini-batch GD\n",
    "\n",
    "- Take your mini-batch\n",
    "- Compute mean and variance just on this mini-batch\n",
    "\n",
    "\n",
    "One important detail:\n",
    "\n",
    "- In typical setting we always have $Z = WX+b$ \n",
    "- But when you think about it, since we normalize Zs by substracting the mean $b$ is kind of useless because it will be subtracted from all samples (it is same for all samples).\n",
    "- For this reason we can think of $\\beta^{[l]}$ playing the role of our good old bias parameter $b$, since:\n",
    "\n",
    "$$\n",
    "Z_{norm}^{[l]} = \\gamma^{[l]} * Z_{norm}^{[l]} + \\beta^{[l]}\n",
    "$$\n",
    "\n",
    "So when we apply batch norm, we can actually remove the bias parameter.\n",
    "\n",
    "\n",
    "Remember that gamma and beta are separate values for EACH hidden unit (so they are arrays with one learnable parameter for each hidden unit!! You are normalizing each hidden unit separately across the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d470c",
   "metadata": {},
   "source": [
    "## Why does Batch Norm work? \n",
    "\n",
    "Makes your intermediate layers more robust to changes in the inputs.\n",
    "\n",
    "To set context: Imagine you trained your cat classifier on black cats. However during test time you have colored cat pictures. This is referred to as Covariate shift: Even though maybe the underlying function (being cat vs non-cat) did not change, distribution of your data changes. This reduces the performance of your model.\n",
    "\n",
    "\n",
    "Now think of covariate shift happening at each iteration:\n",
    "\n",
    "- In a deep neural network, imagine the fourth layer. After each backpropagation, layers 1 to 3 have updated their weights. That means that the input to layer 4 will change at each iteration.\n",
    "\n",
    "- By applying batch norm to the output of previous layers, we can not make the values have the same distribution. But at least we can ensure that they have a similar scale (mean and variance).\n",
    "\n",
    "- This helps with training each layer more independently from oscillations in former layers and help with speed up.\n",
    "\n",
    "- Make your hidden layers more robust to changes.\n",
    "\n",
    "\n",
    "**Batch Norm as regularization**\n",
    "\n",
    "Secondary benefit of batch norm: \n",
    "\n",
    "- Since the normalization is applied on mini-batches, each mini-batch will have slightly different mean and variance. \n",
    "- This kinda acts as dropout in that, it will introduce some noise to each layer output. \n",
    "- This can be seen as a way of introducing regularization to your network.\n",
    "- Increasing mini-batch size you would reduce the regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554e528",
   "metadata": {},
   "source": [
    "## Batch Norm at Test Time\n",
    "\n",
    "Maybe in prediction time our mini-batch size is 1. What will we do??\n",
    "\n",
    "\n",
    "Different way of calculating $\\mu$ and $\\sigma$ are needed since we dont have a mini-batch now.\n",
    "\n",
    "In typical applications: \n",
    "\n",
    "- Estimate them using a seperate exponentially weighted average (across mini-batches): calculate the running average of means and variance during training.\n",
    "\n",
    "\n",
    "- During test time, use these running average values of $\\mu$ and $\\sigma$ values to calculate the normalization following the same batch norm formula.\n",
    "\n",
    "\n",
    "In practice, apparently batch norm is quite robust to minor changes in these values so no need to worry too much about the exact methodology in how we estimate these numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204982ef",
   "metadata": {},
   "source": [
    "# Part 3. Multi-class Classification\n",
    "\n",
    "## Softmax Regression\n",
    "\n",
    "Softmax regression is a way to go up to N class from 0-1 binary classification.\n",
    "\n",
    "Well softmax is something we knooow very well no need to take massive notes here I believe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Z_l = np.random.randn(4,1) # last layer output without activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6661645",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_zl = np.exp(Z_l)\n",
    "soft_zl = soft_zl / (np.sum(soft_zl)) # this becomes the last layer activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_zl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_zl.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0f9f8",
   "metadata": {},
   "source": [
    "- Unusual thing about this activation function is that it is not element-wise anymore.\n",
    "\n",
    "- Since we have to normalize all exponentials by the total activation takes as input a (4,1) vector and output (4,1) vector. This is different than sigmoid which receives a single value.\n",
    "\n",
    "\n",
    "You can use a zero-hidden layer neural network with softmax function to have many linear decision boundaries for e.g. 3 classes. \n",
    "\n",
    "$$\n",
    "y_{pred} = softmax(WX+b)\n",
    "$$\n",
    "\n",
    "The decision boundaries will always be linear since we only did linear combination of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e9b9c",
   "metadata": {},
   "source": [
    "## Training a Softmax Classifier\n",
    "\n",
    "Deepen how we can train a neural network.\n",
    "\n",
    "Name is coming from difference from \"hard-max\" in which case we would just get $[1,0,0,0]$ if first element had the highest value. \n",
    "\n",
    "Actually when number of classes (c) equals to 2, softmax actually reduces to logistic regression. So softmax is a generalization of logistic regression to N classes.\n",
    "\n",
    "Convention:\n",
    "\n",
    "$$\n",
    "Z^{[L]} = [5,2,-1,3]\\\\\n",
    "t = [e^5,e^2,e^{-1},e^3]\n",
    "$$\n",
    "\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "target output $y = [0,1,0,0]$, prediction $y_{pred}  = [0.3,0.2,0.1,0.4]$\n",
    "\n",
    "$$\n",
    "L(y_{pred},y) = - \\sum_{j=1}^4 y_j * log(y_{pred j})\\\\\n",
    "= -y_2 * log(y_{pred 2}) =  -log(y_{pred 2})\n",
    "$$\n",
    "\n",
    "So we should make $y_{pred 2}$ as close to 1 as possible.\n",
    "\n",
    "Reminder that each of your target $y$ are one-hot encoding of the target class for that sample.\n",
    "\n",
    "Loss with respect to output of the last layer (before softmax):\n",
    "$$\n",
    "dZ^{[L]}  = y_{pred} - y\n",
    "$$\n",
    "\n",
    "where $y_{pred}$ is the softmax output vector and $y$ is the one-hot encoding target vector!!\n",
    "\n",
    "**!!!!   DO NOT SKIP THE DERIVATION OF THE SOFTMAX GRADIENT   !!!!**\n",
    "\n",
    "READ: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ff9b9",
   "metadata": {},
   "source": [
    "### (Bonus) Proof for Derivative through Softmax ($\\dfrac{\\partial \\mathcal{L}}{\\partial Z}$)\n",
    "\n",
    "First to proof to make sense let's agree on some notation. Let's consider an example where we have 4 classes and the correct answer is class 3. So $Y$ will look like this:\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "Z = WX+B\\\\\n",
    "A = \\text{softmax}(Z)\\\\\n",
    "\\mathcal{L} = - \\sum_{i=1}^4 Y_i \\cdot log(A_i)  = - log(A_3)\n",
    "$$\n",
    "\n",
    "last equality follows from Y being a one-hot encoding with only $Y_3=1$ as shown above.\n",
    "\n",
    "We are interested in knowing $\\dfrac{\\partial \\mathcal{L}}{\\partial Z}$. Recall that we can use the chain rule to get this value:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\mathcal{L}}{Z} = \\dfrac{\\partial \\mathcal{L}}{\\partial A} \\cdot \\dfrac{\\partial A}{\\partial Z}\n",
    "$$\n",
    "\n",
    "**Step 1. $\\dfrac{\\partial \\mathcal{L}}{\\partial A}$**\n",
    "\n",
    "\n",
    "This step is trivial since we already shown the derivation above. Since it only depends on $A_3$ it will look like this:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial A} = \\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "-\\dfrac{1}{A_3}\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "since $\\dfrac{\\partial log(x)}{\\partial x}=\\dfrac{1}{x}$.\n",
    "\n",
    "\n",
    "**Step 2. Derivative for Softmax**\n",
    "\n",
    "Recall softmax operation $s(A)_i = \\dfrac{e^{A_i}}{\\sum_{j=1}^N e^{A_j}}$.  And we are interested in the derivative of $s(A)_3$ with respect to all $A_j$ where $j=\\{1,2,3,4\\}$ (as other values will cancel out since dl/dA being zero for those indices).\n",
    "\n",
    "For this we will need the quotient rule.\n",
    "\n",
    "##### Recall Quotient Rule.\n",
    "\n",
    "$$\n",
    "f(x) = \\dfrac{g(x)}{h(x)}\\\\\n",
    "f^{'}(x) = \\dfrac{g^{'}(x)\\cdot h(x) - h^{'}(x)\\cdot g(x)}{[h(x)]^2}\n",
    "$$\n",
    "\n",
    "For simplicity lets refer to denominator of softmax ($\\sum_{i} e^{A_i}$) as $T$ and $s(A)_j$ simply as $S_j$ so $S_j=\\dfrac{e^{A_j}}{T}$. \n",
    "\n",
    "Applying the Quotient rule to softmax for each index $j$ to get $\\dfrac{\\partial s(A)_3}{\\partial A_j}$.\n",
    "\n",
    "\n",
    "**Case 1.$j=3$**\n",
    "\n",
    "\n",
    "Then the quotient rule gives:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{e^{A_3} * T - e^{A_3}\\cdot e^{A_3}}{T^2} &= \\dfrac{e^{A_3}\\cdot (T-e^{A_3})}{T^2}\\\\ &= \\dfrac{e^{A_3}}{T} \\cdot \\dfrac{T - A_3}{T}\\\\ &= S_3 \\cdot (1-S_3)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "**Case 1.$j=3$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ce7f1",
   "metadata": {},
   "source": [
    "# Part 4. Intro to Programming Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa35d4",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks\n",
    "\n",
    "More practical to use some deep learning frameworks.. So old it is mentioning stuff like Caffe instead of just TensorFlow and PyTorch.\n",
    "\n",
    "Criteria:\n",
    "\n",
    "- Ease of programming (development and deployment)\n",
    "- Running speed\n",
    "- Truly open (open source with good governance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46741f00",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "Super basic structure and intro to TensorFlow. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f52251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 21:31:53.875550: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec32c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = w**2 - 10*w + 25\n",
    "    trainable_variables = [w]\n",
    "    grads = tape.gradient(cost,trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,trainable_variables))\n",
    "\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e182f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999931>\n"
     ]
    }
   ],
   "source": [
    "train_step()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6217bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    train_step()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa6a3e",
   "metadata": {},
   "source": [
    "And voila!! with only using the gradientTape thing and an optimizer which we give input the trainable parameter $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f082e7",
   "metadata": {},
   "source": [
    "To summarize the above: \n",
    "\n",
    "- We declare the cost function within GradientTape context.\n",
    "- Then calculate grads for the trainable params, given the cost we want to minimize.\n",
    "- Finally, apply the gradients using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e76e7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999931>\n"
     ]
    }
   ],
   "source": [
    "# Another function\n",
    "# What if we have both input (x) and parameters (w)\n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "x = np.array([1.0,-10,25.0],dtype=np.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "def cost_fn():\n",
    "    cost = x[0]*w**2 + x[1]*w + x[2]\n",
    "    return cost\n",
    "\n",
    "print(w)\n",
    "optimizer.minimize(cost_fn,[w])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ae8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.minimize() is an alternative representation \n",
    "# of the first implementation\n",
    "# Similar to what we built in micrograd with Karpathy\n",
    "def training(x,w,optimizer):\n",
    "    def cost_fn():\n",
    "        cost = x[0]*w**2 + x[1]*w + x[2]\n",
    "        return cost\n",
    "    for i in range(1000):\n",
    "        optimizer.minimize(cost_fn,[w])\n",
    "        \n",
    "    return w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c73c0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0000005>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(x,w,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cd284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-karpathy",
   "language": "python",
   "name": "venv-karpathy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
