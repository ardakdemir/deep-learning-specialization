{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f4fd34",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304d21b",
   "metadata": {},
   "source": [
    "## Computer Vision\n",
    "\n",
    "- CV is helping self-driving cars \n",
    "- Face recognition!!!\n",
    "- DL for retrieval of relevant images\n",
    "- Art!\n",
    "\n",
    "Andrew is excited about CV because:\n",
    "\n",
    "- Rapid advance open up brand new opportunities\n",
    "- CV research community is very creative and inventive for other disciplines\n",
    "- Speech Recognition borrowed ideas from CV community\n",
    "\n",
    "\n",
    "Neural style transfer: Input one target image and one image for style: Output is target created in that style e.g. profile pic in Picasso style.\n",
    "\n",
    "\n",
    "High quality image:  \n",
    "- 1000x1000x3 input dimensional.\n",
    "- If you have 1000 hidden units. \n",
    "- Then your W1 will be (1000,3M) 3B parameters only in one layer looool.\n",
    "- Convolution comes to the rescue!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bf339",
   "metadata": {},
   "source": [
    "## Edge Detection Example\n",
    "\n",
    "Previous discussion:\n",
    "- Edge in early layer\n",
    "- Facial features in mid layer\n",
    "- Faces in upper layers\n",
    "\n",
    "Lets learn about step 1: Edge detection\n",
    "\n",
    "\n",
    "- Vertical edges, horizontal edges\n",
    "\n",
    "\n",
    "#### Vertical Edge Detection\n",
    "\n",
    "Have a 3x3 kernel: \n",
    "\n",
    "$$\n",
    "[1,0,-1]\\\\\n",
    "[1,0,-1]\\\\\n",
    "[1,0,-1]\n",
    "$$\n",
    "\n",
    "and apply convolution to your image (6x6). Why??\n",
    "\n",
    "Imagine input image with clear vertical edge:\n",
    "\n",
    "$$\n",
    "[10,10,10,0,0,0]\\\\\n",
    "[10,10,10,0,0,0]\\\\\n",
    "[10,10,10,0,0,0]\\\\\n",
    "[10,10,10,0,0,0]\\\\\n",
    "[10,10,10,0,0,0]\\\\\n",
    "$$\n",
    "\n",
    "- Bright pixels will have high values.\n",
    "- Dark pixels will have low or even negative values.\n",
    "- If we convolve this matrix over an image, we get very positive number when there is a transition from bright to dark (left -> right).\n",
    "- Opposite case if its right -> left than it will have negative numbers. \n",
    "- Results will look like this:\n",
    "\n",
    "\n",
    "$$\n",
    "[0,30,30,0]\\\\\n",
    "[0,30,30,0]\\\\\n",
    "[0,30,30,0]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a956caa",
   "metadata": {},
   "source": [
    "## More Edge Detection\n",
    "\n",
    "- Positive and negative images\n",
    "- Other edge detectors\n",
    "- How network can learn detecting the edges rather than hand-coding them.\n",
    "\n",
    "\n",
    "Horizontal edge detector:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 0 & 0\\\\\n",
    "-1 & -1 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "- There was a lot of debate on what the best edge detector values should be??\n",
    "- Some vertical edge detectors below:\n",
    "\n",
    "Sobel Filter:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1\\\\\n",
    "2 & 0 & -2\\\\\n",
    "1 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Scharr filter: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & -3\\\\\n",
    "10 & 0 & -10\\\\\n",
    "3 & 0 & -3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "In deep learning era, we can maybe learn the 9 values without handcoding them!\n",
    "\n",
    "- Whether hand-coded or learned, convolution is the key. \n",
    "- Maybe we can even learn 45 degreee, 75 degree edges by not hard coding them: More flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae456ba",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "One of the building blocks of convolution layers.\n",
    "\n",
    "- Input is 6x6 and filter is 3x3 then output is 4x4\n",
    "- Formula is n-f+1 x n-f+1 (stride is one).\n",
    "\n",
    "Downsides:\n",
    "- With every operation your image shrinks\n",
    "- The pixels on the corner or edges are used only once. Whereas the ones in the middle are used multiple times. As if we put less importantce on the corner/edges which is not true..\n",
    "\n",
    "Solution:\n",
    "\n",
    "- Pad the image in both directions (up-down and left right)\n",
    "- 6x6 -> 8x8 by convention pad by zeros (when p=1)\n",
    "- General formula: n+2p-f+1 x  n+2p-f+1\n",
    "- So output will be 6x6 after we apply 3x3 filter, yay!!\n",
    "\n",
    "\n",
    "\n",
    "**Some terminology**\n",
    "\n",
    "Valid convolution: (no padding) nxn  * fxf     -> n-f+1 x n-f+1\n",
    "\n",
    "Same convolution: \n",
    "\n",
    "- Pad so that output size is the same as the input size.\n",
    "- To ensure we have same shape after convolution we need padding of size $p=\\dfrac{f-1}{2}$\n",
    "\n",
    "$$\n",
    "n+2p-f+1 = n\\\\\n",
    "2p = f-1\\\\\n",
    "p = \\dfrac{f-1}{2}\n",
    "$$\n",
    "\n",
    "\n",
    "NOTE: It is nice to have odd number filters (3,3) (5,5) so we can talk about \"center\" of a filter. It is very common in computer vision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ae915",
   "metadata": {},
   "source": [
    "## Strided Convolution\n",
    "\n",
    "- Only important reminder is the stride is taken in both vertical and horizontal directions.\n",
    "- If stride=2 we shift the filter two steps first in horizontal until we reach the end.\n",
    "- Then we shift two step down and start from left and repeat.\n",
    "\n",
    "Formula including stride for size of the output:\n",
    "\n",
    "$$\n",
    "n_{out} = \\lfloor \\dfrac{n+2p-f}{s} + 1 \\rfloor\n",
    "$$\n",
    "\n",
    "\n",
    "Last reminder: Sometimes signal processing folks refer to convolution as the cross-correlation operation. Which flips the filter in both axis and then apply convolution. We don't bother to do that in deep learning.\n",
    "\n",
    "filter:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "4 & 5 & 6\\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "flipped version: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "9 & 8 & 7\\\\\n",
    "6 & 5 & 4\\\\\n",
    "3 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We dont do such thing in computer vision but just fyi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de85827f",
   "metadata": {},
   "source": [
    "## Convolutions Over Volume\n",
    "\n",
    "Two main points of this video\n",
    "\n",
    "- If input is a volume (e.g., 6x6x3 as in RGB) filters should also have 3 channels e.g. 3x3 filter should now become 3x3x3 (one filter for each channel).\n",
    "- When we apply multiple filters to an input, we stack the convolution results as channels. So if there is a 6x6x2 image (two channels) and we apply two 3x3x2 filters, then the output is 4x4x2.\n",
    "     - This convention is helpful because subsequent layers will also have same design: volume in, volume out.\n",
    "     \n",
    "Beware that even when input is a volume, the output of ONE filter is a matrix:\n",
    "\n",
    "                            6x6x3  *  3x3x3 ----> 4x4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12cf2f",
   "metadata": {},
   "source": [
    "## One Layer of a Convolutional Network\n",
    "\n",
    "\n",
    "Last steps to convert our filter outputs to Neural Network\n",
    "\n",
    "\n",
    "**One layer of Convolution:**\n",
    "\n",
    "- Add a single bias number to all cells of the output of a filter\n",
    "- Apply ReLU (or some non-linear activation) to the output of step one.\n",
    "\n",
    "\n",
    "$$\n",
    "a_{1} = ReLU( \\begin{bmatrix}\n",
    "9 & 8 & 7\\\\\n",
    "6 & 5 & 4\\\\\n",
    "3 & 2 & 1\n",
    "\\end{bmatrix} + b_1 ) \\\\\n",
    "a_{2} = ReLU( \\begin{bmatrix}\n",
    "9 & 8 & 7\\\\\n",
    "6 & 5 & 4\\\\\n",
    "3 & 2 & 1\n",
    "\\end{bmatrix} + b_1 )\\\\\n",
    "a^{[1]} = [a_1,a_2]_{(3x3x2)}\n",
    "$$\n",
    "\n",
    "If we had 10 filters then the output of this convolution layer would be 3x3x10.\n",
    "\n",
    "\n",
    "Imagine you had 3x3x3 10 filters you only have 280 parameters. Potentially you can apply this to huuuge images. So convolution is super parameter efficient.\n",
    "\n",
    "\n",
    "**Notation**\n",
    "\n",
    "$$\n",
    "f^{[l]} = \\text{filter size}\\\\\n",
    "p^{[l]} = \\text{padding}\\\\\n",
    "s^{[l]} = \\text{stride}\\\\\n",
    "n_c^{[l]} = \\text{number of filters}\\\\\n",
    "\\text{Each filter is: }  f^{[l]} \\times f^{[l]} \\times n_c^{l-1}\\\\\n",
    "\\text{Input dims (for layer l)} = n_{H}^{[l-1]} \\times n_{W}^{[l-1]} \\times n_c^{[l-1]}\\\\\n",
    "\\text{Output dims (of layer l)} =  n_{H}^{[l]} \\times n_{W}^{[l]} \\times n_c^{[l]}\\\\\n",
    "\\text{(Batch output)} A^{[l]}  \\rightarrow m \\times  n_{H}^{[l]} \\times n_{W}^{[l]} \\times n_c^{[l]}\\\\\n",
    "\\text{Weights: }  f^{[l]} \\times f^{[l]} \\times n_c^{l-1} \\times n_c^{l} \\\\\n",
    "\\text{Bias:   }  n_c^{[l]}    \\rightarrow (1,1,1,n_c^{[l]}) \\text{  (better to shape it like this)}\n",
    "$$\n",
    "\n",
    "\n",
    "Key takeaway is to just have some convention of how to represent each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163112a",
   "metadata": {},
   "source": [
    "## Simple Convolution Neural Network\n",
    "\n",
    "**Example ConvNet** for cat classifier\n",
    "\n",
    "Lets say input image is 39x39x3.\n",
    "\n",
    "- First layer f=3 s=1 p=0 10 filters -> output will be 37x37x10\n",
    "- Second layer f=5 s=2 p=0 20 filters -> output will be 17x17x20\n",
    "- Third layer f=5, s=2, p=0 40 filters -> output 7x7x40 \n",
    "- Finally, flatten it to have 1960 units \n",
    "- Feed to logistic regression/softmax layer\n",
    "\n",
    "\n",
    "Wow so many things to tune such as number of filters, filter size etc.\n",
    "\n",
    "Some guideline:\n",
    "\n",
    "- Usually start with large image\n",
    "- Size gradually shrinks\n",
    "- Whereas the channel size gradually increases\n",
    "\n",
    "\n",
    "Types of layers in a Convolutional Network:\n",
    "\n",
    "- Convolution layer (Conv)\n",
    "- Pooling (Pool)\n",
    "- Fully Connected (FC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b6340",
   "metadata": {},
   "source": [
    "## Pooling Layers \n",
    "\n",
    "- Reduce size of the representation to speed up computation\n",
    "- Also make feature detection more robust.\n",
    "\n",
    "Lets see why.\n",
    "\n",
    "**Max Pooling**\n",
    "\n",
    "- 4x4 output of the convolution layer is input\n",
    "- Separate it into 4 2x2 (filter size) disjoint regions (determined by stride)\n",
    "- Output is max of each region as 2x2\n",
    "- filter size and stride sizes are also hyperparameters of the pooling layer\n",
    "\n",
    "Example for the above max pooling:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "9&3&4&0\\\\\n",
    "1&2&5&6\\\\\n",
    "2&1&4&11\\\\\n",
    "2&3&-1&5\n",
    "\\end{bmatrix}    \\rightarrow_{max pooling} \\begin{bmatrix}\n",
    "9&6\\\\\n",
    "3&11\\\\\n",
    "\\end{bmatrix}  \n",
    "$$ \n",
    "\n",
    "\n",
    "Why is this helpful? \n",
    "\n",
    "- If the feature is detected anywhere (high value) keep it\n",
    "- If the feature doesn't exist at all after max pooling we won't have it represented...\n",
    "\n",
    "\n",
    "Note: Max pooling has hyperparameters BUT doesn't have ANY learnable parameters. It is simply maxing the previous layer output.\n",
    "\n",
    "\n",
    "\n",
    "**Important!!** If the input to the pooling layer has N channels, the output of the pooling layer will also be N channeled. Maxing is applied to each of the N channels separately!!\n",
    "\n",
    "\n",
    "\n",
    "**Average pooling** \n",
    "\n",
    "- It is used waaay less often in CNNs.\n",
    "- Sometimes used to flatten the volumes.\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "- filter size f=2,3 very common\n",
    "- stride s=2 common\n",
    "- max or average pooling\n",
    "- padding is very rarely used!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209e78b",
   "metadata": {},
   "source": [
    "## CNN Example\n",
    "\n",
    "\n",
    "Andrew goes over each layer in terms of input output dim sizes of LeNet-5 by Yann LeCunn.\n",
    "\n",
    "- Sometimes we consider CONV+POOL as one layer\n",
    "- Some people separate each as separate layers\n",
    "- Common to only refer to layers that have trainable parameters (POOL doesn't have)..\n",
    "\n",
    "\n",
    "Some important guidelines:\n",
    "\n",
    "- Do not try to come up with your own hyperparameters since there are toooo many things to try. Look at other works and use them as reference such as architectures that are proven to be useful.\n",
    "- It is usually considered better that the shape and size of each layer change gradually.\n",
    "- If the output is 1 dimensional or 10 dimensional, it is much better to shrink the outputs gradually as opposed to suddenly going down from 1200 activations to 10 or 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb874588",
   "metadata": {},
   "source": [
    "## Why Convolutions??\n",
    "\n",
    "Two main advantages\n",
    "\n",
    "- Parameter sharing!\n",
    "- Sparsity of connections!\n",
    "\n",
    "**Parameter Sharing**: A feature detector (such as a vertical edge detector) that's useful in one part of the image is probably useful in another part of the image.\n",
    "\n",
    "**Sparsity of connections**: Means that unlike FC, not all previous layer output is considered when we generate the activation of the next layer. In each layer, each output value depends only on a small number of inputs.\n",
    "\n",
    "\n",
    "Imagine 32x32x3 input to f=5 and 6 filter --> 28x28x6.\n",
    "\n",
    "If you think about the params, this means 3072 input and 4704 output numbers. If we were to have FC layer of these input output sizes, we would need 14M parameters. Now we just have $(5 x 5x3+1)x6=456$!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44c872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375b777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f528d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f9f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847e4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980f123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614aee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c605d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed631ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb75808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10bc73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f51157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe524a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f2596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112d48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff9ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c0dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7683c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22866e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053d70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da9fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-karpathy",
   "language": "python",
   "name": "venv-karpathy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
